# BudgetBuddy - Personal Finance Chatbot üí∞

**Domain-Specific Chatbot using Fine-Tuned FLAN-T5 + RAG**

BudgetBuddy is a finance assistant that answers questions about personal spending, budgets, and transactions. Built with fine-tuned FLAN-T5, FAISS vector search, and a hybrid question-answering pipeline.


##  Features

- **Fine-Tuned FLAN-T5**: Custom-trained on 340+ financial Q&A pairs
- **RAG System**: FAISS vector database with 806 transaction embeddings
- **Hybrid Pipeline**: Combines rule-based extraction + RAG + generative AI
- **Smart Query Routing**: Detects aggregate queries vs. contextual questions
- **Gradio Interface**: User-friendly web UI for interactive queries
- **Out-of-Domain Detection**: Filters non-finance questions

---

##  Dataset

**Personal Transactions (2018)**
- **806 transactions** from January - December 2018
- **688 debits** ($96,083.78) | **118 credits** ($124,269.76)
- **17+ categories**: Groceries, Entertainment, Utilities, Mortgage, etc.

**Generated Training Data**
- **340 question-answer pairs** synthesized from transactions
- Questions cover spending by category, time period, and overall patterns

---

##  Project Structure

```
DOMAIN-SPECIFIC-CHATBOT/
‚îú‚îÄ‚îÄ app.py                          # Deployment script (Gradio web app)
‚îú‚îÄ‚îÄ chatbot.ipynb                   # Complete training pipeline
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ RESULTS_SUMMARY.md             # Training metrics and evaluation
‚îú‚îÄ‚îÄ DEMO_SCRIPT.md                 # Live demo presentation guide
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ personal_transactions new.csv
‚îú‚îÄ‚îÄ saved_models/                  # Generated by running notebook
‚îÇ   ‚îú‚îÄ‚îÄ fine_tuned_t5/             # Fine-tuned FLAN-T5 model (not in repo - too large)
‚îÇ   ‚îú‚îÄ‚îÄ transactions.faiss         # FAISS vector index (auto-generated)
‚îÇ   ‚îî‚îÄ‚îÄ faiss_meta.pkl            # Transaction metadata (auto-generated)
‚îî‚îÄ‚îÄ .venv/                         # Virtual environment (local)
```

**Note:** Model files in `saved_models/` are not included in the repository due to size constraints (419 MB). The app will:
- Use base FLAN-T5 if fine-tuned model is missing
- Auto-generate FAISS index from CSV data if missing
- For best results, run `chatbot.ipynb` to create the fine-tuned model locally

---

##  Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/lmurayire12/DOMAIN-SPECIFIC-CHATBOT.git
cd DOMAIN-SPECIFIC-CHATBOT
```

### 2. Set Up Virtual Environment

```bash
python -m venv .venv
.venv\Scripts\activate          # Windows
source .venv/bin/activate       # Linux/Mac
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. (Optional) Train the Model

To get the best results, train the model first:

```bash
# Open and run all cells in chatbot.ipynb
# This will create the fine-tuned model in saved_models/
```

**Note:** The app works with the base FLAN-T5 model if you skip this step, but responses will be less accurate.

### 5. Run the Chatbot

```bash
python app.py
```

Open your browser at **http://localhost:7860**

**First-time setup:**
- If model files are missing, the app will use the base FLAN-T5 model
- If FAISS index is missing, it will build one automatically from the CSV data
- For best results, run the notebook first to create fine-tuned model

---

##  Notebook Walkthrough

`chatbot.ipynb` contains the complete pipeline:

### Section 1: Setup
1. **Install Required Libraries** - TensorFlow, Transformers, FAISS, Gradio
2. **Import Dependencies** - All necessary packages
3. **Set Configuration Parameters** - Model paths, hyperparameters

### Section 2: Data Processing
4. **Load and Clean Transaction Data** - Parse dates, amounts, categories
5. **Generate Training Dataset** - Create 340 Q&A pairs from transactions
6. **Split Dataset** - 90/10 train/validation split

### Section 3: Model Training
7. **Prepare Tokenizer and Data Pipeline** - FLAN-T5 tokenizer, TensorFlow datasets
8. **Load FLAN-T5 Model** - `google/flan-t5-small` (60M parameters)
9. **Train the Model** - 3 epochs, Adam optimizer, learning rate 5e-5
10. **Evaluate Performance** - BLEU: 0.044, ROUGE-1: 0.012

### Section 4: RAG System
11. **Build FAISS Vector Database** - Embed 806 transactions with MiniLM-L6-v2
12. **Test RAG Retrieval** - Query similarity search
13. **Create Rule-Based Handler** - Date parsing, category extraction

### Section 5: Deployment
14. **Install dateparser** - Natural language date parsing
15. **Build Hybrid Pipeline** - Combine rule-based + RAG + generative
16. **Launch Gradio Interface** - Interactive web UI
17. **Copy Model Files** - Organize for deployment
18. **Generate app.py** - Standalone deployment script

---

## Training Results

### Model Performance
- **Base Model**: `google/flan-t5-small` (60M parameters)
- **Training Time**: ~45 minutes (3 epochs)
- **Final Loss**: 1925.48 (76% reduction from 8132.98)
- **Validation Loss**: 1769.57

### Evaluation Metrics
| Metric | Score |
|--------|-------|
| BLEU | 0.044 |
| ROUGE-1 | 0.012 |
| ROUGE-2 | 0.006 |
| ROUGE-L | 0.012 |

### Training Configuration
```python
BATCH_SIZE = 8
EPOCHS = 3
LEARNING_RATE = 5e-5
MAX_INPUT_LENGTH = 256
MAX_TARGET_LENGTH = 128
```

---

##  Example Queries

```python
# Aggregate spending query
"How much did I spend on Entertainment in May 2018?"
‚Üí "You spent $182.17 on Entertainment in May 2018, mostly on movie tickets."

# Top categories
"What are my top spending categories?"
‚Üí "Your top 3 categories are: Credit Card Payment, Mortgage, Home Improvement."

# Contextual question (RAG)
"Why did my spending increase last month?"
‚Üí Uses FAISS to retrieve relevant transactions and generates contextual answer.

# Time-based query
"Show me transactions from January 2018"
‚Üí Retrieves and summarizes all January transactions.
```

---

## üõ†Ô∏è Technology Stack

| Component | Technology |
|-----------|-----------|
| **Base Model** | FLAN-T5-small (Google) |
| **Framework** | TensorFlow 2.15 |
| **Vector DB** | FAISS (IndexFlatIP) |
| **Embeddings** | sentence-transformers/all-MiniLM-L6-v2 |
| **UI** | Gradio 4.8.0 |
| **NLP Tools** | dateparser, regex |
| **Data Processing** | Pandas, NumPy |

---

## Requirements

Full dependencies in `requirements.txt`:

```txt
transformers==4.35.0
sentence-transformers==2.2.2
faiss-cpu==1.7.4
tensorflow==2.15.0
gradio==4.8.0
pandas==2.1.3
numpy==1.26.2
dateparser==1.2.0
sacrebleu
rouge_score
```

---

##  How It Works

### Hybrid Question-Answering Pipeline

1. **Intent Detection**
   - Checks if question is finance-related
   - Identifies aggregate queries (spending calculations)

2. **Query Routing**
   - **Aggregate queries** ‚Üí Rule-based extraction + SQL-like filtering
   - **Contextual queries** ‚Üí FAISS retrieval + T5 generation

3. **Answer Generation**
   - Structured data: Direct computation from DataFrame
   - Unstructured queries: RAG pipeline with top-4 similar transactions
   - Context injection: `{question} </s> CONTEXT: {retrieved_docs}`

4. **Response Formatting**
   - Natural language with specific amounts
   - Category breakdowns and time periods
   - Personalized insights
